{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-f6bfdb7cb74b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-f6bfdb7cb74b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    saver = tf.train.Saver(...variables...)\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Create a saver.\n",
    "saver = tf.train.Saver(...variables...)\n",
    "# Launch the graph and train, saving the model every 1,000 steps.\n",
    "sess = tf.Session()\n",
    "for step in xrange(1000000):\n",
    "    sess.run(..training_op..)\n",
    "    if step % 1000 == 0:\n",
    "        # Append the step number to the checkpoint name:\n",
    "        saver.save(sess, 'my-model', global_step=step)\n",
    "\n",
    "# to retrieve the saved tensorflow model:\n",
    "# After this, the value of tensors like w1 and w2 has been restored and can be accessed:\n",
    "with tf.Session() as sess:\n",
    "  new_saver = tf.train.import_meta_graph('my_test_model-1000.meta')\n",
    "  new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "  print(sess.run('w1:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from pathlib import Path\n",
    " \n",
    "#Prepare to feed input, i.e. feed_dict and placeholders\n",
    "w1 = tf.placeholder(\"float\", name=\"w1\")\n",
    "w2 = tf.placeholder(\"float\", name=\"w2\")\n",
    "b1= tf.Variable(2.0,name=\"bias\")\n",
    "feed_dict ={w1:4,w2:8}\n",
    " \n",
    "#Define a test operation that we will restore\n",
    "w3 = tf.add(w1,w2)\n",
    "w4 = tf.multiply(w3,b1,name=\"op_to_restore\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    " \n",
    "#Create a saver object which will save all the variables\n",
    "saver = tf.train.Saver()\n",
    " \n",
    "#Run the operation by feeding input\n",
    "print(sess.run(w4,feed_dict))\n",
    "#Prints 24 which is sum of (w1+w2)*b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\real_summaries\\my_test_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\real_summaries\\\\my_test_model-1000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, save the graph\n",
    "logdir = 'C:/real_summaries'\n",
    "checkpoint_file = os.path.join(Path(logdir), 'my_test_model')\n",
    "print(checkpoint_file)\n",
    "# print(os.path.pardir) # this print: .. (current directory)\n",
    "saver.save(sess, checkpoint_file,global_step=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\real_summaries\n"
     ]
    }
   ],
   "source": [
    "#How to access saved variable/Tensor/placeholders \n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\") # w1 is a tensor variable without value, for this you must execute sess.run(var,feed_dict={w1:4,w2:8}).\n",
    "# print(w1.eval(session=tf.Session())) # this works only after a run operation.\n",
    " \n",
    "## How to access saved operation\n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "# print(op_to_restore.eval(session=tf.Session()))\n",
    "print(Path(logdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\real_summaries\\my_test_model-1000.meta\n",
      "INFO:tensorflow:Restoring parameters from C:\\real_summaries\\my_test_model-1000\n",
      "10.0\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "# to retrive the graph:\n",
    "# to restore the model, add new operations and training with your own data:\n",
    "# the .meta file contains the weights and gradients? YES\n",
    "# how to include the hyperparameters??\n",
    "checkpoint_file = os.path.join(Path(logdir), 'my_test_model-1000.meta')\n",
    "print(checkpoint_file)\n",
    "# with tf.Session() as sess:    \n",
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph(checkpoint_file)\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(Path(logdir)))\n",
    "print(sess.run('w1:0', {w1:10.0, w2:9.0}))\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    " \n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    " \n",
    "print(sess.run(op_to_restore,feed_dict))\n",
    "#This will print 60 which is calculated \n",
    "#using new values of w1 and w2 and saved value of b1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    }
   ],
   "source": [
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    " \n",
    "graph = tf.get_default_graph()\n",
    "w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "feed_dict ={w1:13.0,w2:17.0}\n",
    " \n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    " \n",
    "#Add more to the current graph\n",
    "add_on_op = tf.multiply(op_to_restore,2)\n",
    " \n",
    "print(sess.run(add_on_op,feed_dict))\n",
    "#This will print 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# this is a real world example:\n",
    "saver = tf.train.import_meta_graph('vgg.meta')\n",
    "# Access the graph\n",
    "graph = tf.get_default_graph()\n",
    "## Prepare the feed_dict for feeding data for fine-tuning \n",
    " \n",
    "#Access the appropriate output for fine-tuning\n",
    "fc7= graph.get_tensor_by_name('fc7:0')\n",
    " \n",
    "#use this if you only want to change gradients of the last layer\n",
    "fc7 = tf.stop_gradient(fc7) # It's an identity function\n",
    "fc7_shape= fc7.get_shape().as_list()\n",
    " \n",
    "new_outputs=2\n",
    "weights = tf.Variable(tf.truncated_normal([fc7_shape[3], num_outputs], stddev=0.05))\n",
    "biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "output = tf.matmul(fc7, weights) + biases\n",
    "pred = tf.nn.softmax(output)\n",
    " \n",
    "# Now, you run this with fine-tuning data in sess.run()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
